"""
RAGè¯„ä¼°å™¨ - ç®€åŒ–ç‰ˆæœ¬
å®ç°åŸºæœ¬çš„å¿ å®åº¦ã€ç›¸å…³æ€§ã€å¬å›ç‡è¯„æµ‹
"""
import openai
from typing import List, Dict, Any
from config.settings import settings
import json
import re
from datetime import datetime

class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_API_BASE
        )
    
    def evaluate_faithfulness(self, answer: str, citations: List[Dict]) -> float:
        """è¯„ä¼°ç­”æ¡ˆå¿ å®åº¦ - ç­”æ¡ˆæ˜¯å¦åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡"""
        if not citations:
            return 0.0
        
        context = "\n".join([c.get('content', '') for c in citations])
        
        prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
        
        ä¸Šä¸‹æ–‡:
        {context}
        
        ç­”æ¡ˆ:
        {answer}
        
        è¯„ä¼°æ ‡å‡†:
        - 1.0: ç­”æ¡ˆå®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰è™šå‡ä¿¡æ¯
        - 0.8: ç­”æ¡ˆå¤§éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼Œæœ‰å°‘é‡æ¨ç†
        - 0.6: ç­”æ¡ˆéƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼Œæœ‰ä¸€äº›æ¨ç†å’Œæ‰©å±•
        - 0.4: ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡ç›¸å…³ä½†æœ‰æ˜æ˜¾æ‰©å±•
        - 0.2: ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡å…³ç³»è¾ƒå¼±
        - 0.0: ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡æ— å…³æˆ–åŒ…å«è™šå‡ä¿¡æ¯
        
        è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            score_text = response.choices[0].message.content.strip()
            score = float(re.findall(r'[0-1]\.\d+|[0-1]', score_text)[0])
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            print(f"å¿ å®åº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def evaluate_answer_relevancy(self, question: str, answer: str) -> float:
        """è¯„ä¼°ç­”æ¡ˆç›¸å…³æ€§ - ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜"""
        prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆå¯¹é—®é¢˜çš„ç›¸å…³æ€§ã€‚
        
        é—®é¢˜:
        {question}
        
        ç­”æ¡ˆ:
        {answer}
        
        è¯„ä¼°æ ‡å‡†:
        - 1.0: ç­”æ¡ˆå®Œå…¨ä¸”ç›´æ¥å›ç­”äº†é—®é¢˜
        - 0.8: ç­”æ¡ˆåŸºæœ¬å›ç­”äº†é—®é¢˜ï¼Œæœ‰å°‘é‡åç¦»
        - 0.6: ç­”æ¡ˆéƒ¨åˆ†å›ç­”äº†é—®é¢˜
        - 0.4: ç­”æ¡ˆä¸é—®é¢˜ç›¸å…³ä½†ä¸å¤Ÿç›´æ¥
        - 0.2: ç­”æ¡ˆä¸é—®é¢˜å…³ç³»è¾ƒå¼±
        - 0.0: ç­”æ¡ˆä¸é—®é¢˜æ— å…³
        
        è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            score_text = response.choices[0].message.content.strip()
            score = float(re.findall(r'[0-1]\.\d+|[0-1]', score_text)[0])
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            print(f"ç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    def evaluate_context_recall(self, question: str, citations: List[Dict], ground_truth: str = None) -> float:
        """è¯„ä¼°ä¸Šä¸‹æ–‡å¬å›ç‡ - æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯"""
        if not citations:
            return 0.0
        
        context = "\n".join([c.get('content', '') for c in citations])
        
        prompt = f"""
        è¯·è¯„ä¼°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”ä»¥ä¸‹é—®é¢˜æ‰€éœ€çš„è¶³å¤Ÿä¿¡æ¯ã€‚
        
        é—®é¢˜:
        {question}
        
        æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:
        {context}
        
        è¯„ä¼°æ ‡å‡†:
        - 1.0: ä¸Šä¸‹æ–‡åŒ…å«å®Œå…¨å›ç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
        - 0.8: ä¸Šä¸‹æ–‡åŒ…å«å¤§éƒ¨åˆ†å¿…è¦ä¿¡æ¯
        - 0.6: ä¸Šä¸‹æ–‡åŒ…å«éƒ¨åˆ†å¿…è¦ä¿¡æ¯
        - 0.4: ä¸Šä¸‹æ–‡åŒ…å«å°‘é‡ç›¸å…³ä¿¡æ¯
        - 0.2: ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸é—®é¢˜ç›¸å…³ä½†ä¸è¶³ä»¥å›ç­”
        - 0.0: ä¸Šä¸‹æ–‡ä¸åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯
        
        è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            score_text = response.choices[0].message.content.strip()
            score = float(re.findall(r'[0-1]\.\d+|[0-1]', score_text)[0])
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            print(f"å¬å›ç‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    def evaluate_single_qa(self, qa_data: Dict) -> Dict[str, float]:
        """è¯„ä¼°å•ä¸ªé—®ç­”å¯¹"""
        question = qa_data['question']
        answer = qa_data['answer']
        citations = qa_data.get('citations', [])
        ground_truth = qa_data.get('ground_truth')
        
        # æ‰§è¡Œå„é¡¹è¯„ä¼°
        faithfulness = self.evaluate_faithfulness(answer, citations)
        answer_relevancy = self.evaluate_answer_relevancy(question, answer)
        context_recall = self.evaluate_context_recall(question, citations, ground_truth)
        
        return {
            'faithfulness': faithfulness,
            'answer_relevancy': answer_relevancy,
            'context_recall': context_recall
        }
    
    def evaluate_qa_batch(self, qa_batch: List[Dict]) -> Dict[str, float]:
        """æ‰¹é‡è¯„ä¼°é—®ç­”å¯¹"""
        if not qa_batch:
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'context_recall': 0.0,
                'count': 0
            }
        
        total_scores = {
            'faithfulness': 0.0,
            'answer_relevancy': 0.0,
            'context_recall': 0.0
        }
        
        valid_count = 0
        
        for qa_data in qa_batch:
            try:
                scores = self.evaluate_single_qa(qa_data)
                for metric, score in scores.items():
                    total_scores[metric] += score
                valid_count += 1
                
            except Exception as e:
                print(f"è¯„ä¼°å•ä¸ªQAå¤±è´¥: {e}")
                continue
        
        if valid_count == 0:
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'context_recall': 0.0,
                'count': 0
            }
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_scores = {
            metric: score / valid_count 
            for metric, score in total_scores.items()
        }
        avg_scores['count'] = valid_count
        
        return avg_scores
    
    def generate_evaluation_report(self, eval_results: Dict) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = f"""
# RAGç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š

## è¯„ä¼°æ¦‚è§ˆ
- è¯„ä¼°æ ·æœ¬æ•°: {eval_results.get('count', 0)}
- è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è¯„ä¼°æŒ‡æ ‡

### å¿ å®åº¦ (Faithfulness): {eval_results.get('faithfulness', 0):.3f}
ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé¿å…å¹»è§‰å’Œè™šå‡ä¿¡æ¯ã€‚

### ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevancy): {eval_results.get('answer_relevancy', 0):.3f}
ç­”æ¡ˆæ˜¯å¦ç›´æ¥ä¸”å‡†ç¡®åœ°å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚

### ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall): {eval_results.get('context_recall', 0):.3f}
æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„è¶³å¤Ÿä¿¡æ¯ã€‚

## è¯„ä¼°ç»“è®º

{'ğŸŸ¢ ä¼˜ç§€' if all(score >= 0.8 for score in [eval_results.get('faithfulness', 0), eval_results.get('answer_relevancy', 0), eval_results.get('context_recall', 0)]) else 'ğŸŸ¡ è‰¯å¥½' if all(score >= 0.6 for score in [eval_results.get('faithfulness', 0), eval_results.get('answer_relevancy', 0), eval_results.get('context_recall', 0)]) else 'ğŸ”´ éœ€è¦æ”¹è¿›'}

## æ”¹è¿›å»ºè®®
"""
        
        # æ ¹æ®å…·ä½“åˆ†æ•°ç»™å‡ºå»ºè®®
        if eval_results.get('faithfulness', 0) < 0.7:
            report += "- åŠ å¼ºä¸Šä¸‹æ–‡çº¦æŸï¼Œå‡å°‘æ¨¡å‹çš„è‡ªç”±å‘æŒ¥\n"
        
        if eval_results.get('answer_relevancy', 0) < 0.7:
            report += "- ä¼˜åŒ–æç¤ºè¯ï¼Œè®©æ¨¡å‹æ›´ä¸“æ³¨äºç›´æ¥å›ç­”é—®é¢˜\n"
        
        if eval_results.get('context_recall', 0) < 0.7:
            report += "- æ”¹è¿›æ£€ç´¢ç­–ç•¥ï¼Œæé«˜ç›¸å…³ä¸Šä¸‹æ–‡çš„å¬å›ç‡\n"
        
        return report

# åˆ›å»ºå…¨å±€è¯„ä¼°å™¨å®ä¾‹
rag_evaluator = RAGEvaluator()
