"""
å¢å¼ºç‰ˆRAGé—®ç­”é“¾
é›†æˆHyDEæ£€ç´¢å’ŒReranké‡æ’åº
"""
import openai
from core.vector_store_compatible import vector_store
from core.hyde_retrieval import hyde_retriever
from core.reranker import reranker
from config.settings import settings
from typing import Dict, List
import time
import json

class EnhancedRAGChain:
    """å¢å¼ºç‰ˆRAGé—®ç­”é“¾æ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆRAGé“¾æ¡"""
        self.client = openai.OpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_API_BASE
        )
        
        # æç¤ºæ¨¡æ¿
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. åªåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
4. é€‚å½“å¼•ç”¨æ–‡æ¡£ä¸­çš„åŸæ–‡
5. ç”¨ä¸­æ–‡å›ç­”

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š"""
    
    def calculate_enhanced_confidence(self, source_docs, question, answer, retrieval_method="basic"):
        """å¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—"""
        if not source_docs:
            return 0.1
        
        # åŸºç¡€åˆ†æ•°
        doc_score = min(len(source_docs) / settings.TOP_K, 1.0) * 0.3
        
        # æ£€ç´¢æ–¹æ³•åŠ åˆ†
        method_bonus = 0.1 if retrieval_method == "hyde" else 0.0
        
        # ç›¸å…³æ€§åˆ†æ•°
        relevance_score = 0
        question_words = set(question.lower().split())
        
        for doc in source_docs:
            doc_words = set(doc.page_content.lower().split())
            if question_words and doc_words:
                overlap = len(question_words & doc_words) / len(question_words | doc_words)
                relevance_score += overlap
        
        relevance_score = min(relevance_score / len(source_docs), 1.0) * 0.3 if source_docs else 0
        
        # ç­”æ¡ˆè´¨é‡åˆ†æ•°
        answer_length_score = min(len(answer.split()) / 50, 1.0) * 0.2
        
        # å…³é”®è¯åŒ¹é…åˆ†æ•°
        keyword_score = 0
        for word in question_words:
            if len(word) > 2 and word in answer.lower():
                keyword_score += 1
        keyword_score = min(keyword_score / max(len(question_words), 1), 1.0) * 0.1
        
        total_confidence = doc_score + relevance_score + answer_length_score + keyword_score + method_bonus
        
        return min(max(total_confidence, 0.1), 0.95)
    
    def ask_basic(self, question: str) -> Dict:
        """åŸºç¡€é—®ç­”æ–¹æ³•"""
        print(f"ğŸ”µ ä½¿ç”¨åŸºç¡€æ£€ç´¢æ–¹æ³•")
        start_time = time.time()
        
        try:
            # 1. åŸºç¡€æ£€ç´¢
            source_docs = vector_store.similarity_search(question, k=settings.TOP_K)
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            
            for i, doc in enumerate(source_docs):
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content}")
                citations.append({
                    "chunk_id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source_file", "unknown"),
                    "chunk_index": doc.metadata.get("chunk_id", i),
                    "retrieval_method": "basic"
                })
            
            context = "\n\n".join(context_parts)
            
            # 3. ç”Ÿæˆç­”æ¡ˆ
            full_prompt = self.system_prompt.format(context=context, question=question)
            
            response = self.client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # 4. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self.calculate_enhanced_confidence(source_docs, question, answer, "basic")
            
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return {
                "question": question,
                "answer": answer,
                "citations": citations,
                "confidence": confidence,
                "response_time": response_time,
                "source_count": len(source_docs),
                "tokens_used": tokens_used,
                "retrieval_method": "basic"
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "citations": [],
                "confidence": 0.0,
                "response_time": time.time() - start_time,
                "source_count": 0,
                "tokens_used": 0,
                "retrieval_method": "basic"
            }
    
    def ask_hyde(self, question: str) -> Dict:
        """ä½¿ç”¨HyDEæ£€ç´¢çš„é—®ç­”æ–¹æ³•"""
        print(f"ä¸­ ä½¿ç”¨HyDEæ£€ç´¢æ–¹æ³•")
        start_time = time.time()
        
        try:
            # 1. HyDEæ£€ç´¢
            source_docs = hyde_retriever.hyde_retrieve(question, k=settings.TOP_K)
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            
            for i, doc in enumerate(source_docs):
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content}")
                citations.append({
                    "chunk_id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source_file", "unknown"),
                    "chunk_index": doc.metadata.get("chunk_id", i),
                    "retrieval_method": "hyde"
                })
            
            context = "\n\n".join(context_parts)
            
            # 3. ç”Ÿæˆç­”æ¡ˆ
            full_prompt = self.system_prompt.format(context=context, question=question)
            
            response = self.client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # 4. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self.calculate_enhanced_confidence(source_docs, question, answer, "hyde")
            
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return {
                "question": question,
                "answer": answer,
                "citations": citations,
                "confidence": confidence,
                "response_time": response_time,
                "source_count": len(source_docs),
                "tokens_used": tokens_used,
                "retrieval_method": "hyde"
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "citations": [],
                "confidence": 0.0,
                "response_time": time.time() - start_time,
                "source_count": 0,
                "tokens_used": 0,
                "retrieval_method": "hyde"
            }
    
    def ask_rerank(self, question: str) -> Dict:
        """ä½¿ç”¨é‡æ’åºçš„é—®ç­”æ–¹æ³•"""
        print(f"ğŸŸ  ä½¿ç”¨é‡æ’åºæ–¹æ³•")
        start_time = time.time()
        
        try:
            # 1. æ£€ç´¢æ›´å¤šæ–‡æ¡£
            candidate_docs = vector_store.similarity_search(question, k=settings.TOP_K * 2)
            
            # 2. é‡æ’åº
            ranked_docs = reranker.simple_rerank(question, candidate_docs, top_k=settings.TOP_K)
            
            # 3. æå–æ–‡æ¡£
            source_docs = [doc for doc, score in ranked_docs]
            
            # 4. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            
            for i, (doc, score) in enumerate(ranked_docs):
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content}")
                citations.append({
                    "chunk_id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source_file", "unknown"),
                    "chunk_index": doc.metadata.get("chunk_id", i),
                    "retrieval_method": "rerank",
                    "rerank_score": round(score, 3)
                })
            
            context = "\n\n".join(context_parts)
            
            # 5. ç”Ÿæˆç­”æ¡ˆ
            full_prompt = self.system_prompt.format(context=context, question=question)
            
            response = self.client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # 6. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self.calculate_enhanced_confidence(source_docs, question, answer, "rerank")
            
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return {
                "question": question,
                "answer": answer,
                "citations": citations,
                "confidence": confidence,
                "response_time": response_time,
                "source_count": len(source_docs),
                "tokens_used": tokens_used,
                "retrieval_method": "rerank"
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "citations": [],
                "confidence": 0.0,
                "response_time": time.time() - start_time,
                "source_count": 0,
                "tokens_used": 0,
                "retrieval_method": "rerank"
            }
    
    def ask_enhanced(self, question: str) -> Dict:
        """ä½¿ç”¨HyDE+é‡æ’åºçš„å¢å¼ºé—®ç­”æ–¹æ³•"""
        print(f"é«˜ ä½¿ç”¨HyDE+é‡æ’åºå¢å¼ºæ–¹æ³•")
        start_time = time.time()
        
        try:
            # 1. HyDEæ£€ç´¢è·å–å€™é€‰æ–‡æ¡£
            candidate_docs = hyde_retriever.hyde_retrieve(question, k=settings.TOP_K * 2)
            
            # 2. é‡æ’åº
            ranked_docs = reranker.simple_rerank(question, candidate_docs, top_k=settings.TOP_K)
            
            # 3. æå–æ–‡æ¡£
            source_docs = [doc for doc, score in ranked_docs]
            
            # 4. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            
            for i, (doc, score) in enumerate(ranked_docs):
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content}")
                citations.append({
                    "chunk_id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source_file", "unknown"),
                    "chunk_index": doc.metadata.get("chunk_id", i),
                    "retrieval_method": "hyde+rerank",
                    "rerank_score": round(score, 3)
                })
            
            context = "\n\n".join(context_parts)
            
            # 5. ç”Ÿæˆç­”æ¡ˆ
            full_prompt = self.system_prompt.format(context=context, question=question)
            
            response = self.client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.1,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # 6. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self.calculate_enhanced_confidence(source_docs, question, answer, "hyde+rerank")
            
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return {
                "question": question,
                "answer": answer,
                "citations": citations,
                "confidence": confidence,
                "response_time": response_time,
                "source_count": len(source_docs),
                "tokens_used": tokens_used,
                "retrieval_method": "hyde+rerank"
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "citations": [],
                "confidence": 0.0,
                "response_time": time.time() - start_time,
                "source_count": 0,
                "tokens_used": 0,
                "retrieval_method": "hyde+rerank"
            }
    
    def compare_methods(self, question: str) -> Dict:
        """æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ"""
        print(f"æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•ï¼Œé—®é¢˜: {question}")
        
        methods = {
            "basic": self.ask_basic,
            "hyde": self.ask_hyde,
            "rerank": self.ask_rerank,
            "enhanced": self.ask_enhanced
        }
        
        results = {}
        for method_name, method_func in methods.items():
            print(f"\n--- æµ‹è¯•{method_name}æ–¹æ³• ---")
            results[method_name] = method_func(question)
        
        return {
            "question": question,
            "results": results,
            "comparison": {
                "confidence_ranking": sorted(
                    [(k, v["confidence"]) for k, v in results.items()],
                    key=lambda x: x[1], reverse=True
                ),
                "speed_ranking": sorted(
                    [(k, v["response_time"]) for k, v in results.items()],
                    key=lambda x: x[1]
                )
            }
        }

# åˆ›å»ºå…¨å±€å¢å¼ºç‰ˆRAGé“¾æ¡å®ä¾‹
enhanced_rag_chain = EnhancedRAGChain()
