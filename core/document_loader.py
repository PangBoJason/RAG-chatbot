from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from config.settings import settings
import os
import hashlib
import pickle
import time
from typing import List

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å’Œå¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = os.path.join(os.path.dirname(__file__), "..", "data", "cache")
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            buf = f.read(65536)  # 64KBå—
            while len(buf) > 0:
                hasher.update(buf)
                buf = f.read(65536)
        return hasher.hexdigest()
    
    def _get_cache_path(self, file_hash: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{file_hash}.pkl")
    
    def _load_from_cache(self, file_hash: str) -> List[Document]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£å—"""
        cache_path = self._get_cache_path(file_hash)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    chunks = pickle.load(f)
                print(f"ğŸ“‹ ä»ç¼“å­˜åŠ è½½æ–‡æ¡£å—: {len(chunks)} ä¸ª")
                return chunks
            except Exception as e:
                print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None
    
    def _save_to_cache(self, file_hash: str, chunks: List[Document]):
        """ä¿å­˜æ–‡æ¡£å—åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(file_hash)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(chunks, f)
            print(f"ğŸ’¾ æ–‡æ¡£å—å·²ç¼“å­˜: {len(chunks)} ä¸ª")
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£"""
        print(f"æ­£åœ¨åŠ è½½æ–‡æ¡£: {file_path}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½å™¨
        file_extension = os.path.splitext(file_path)[1].lower()
        
        if file_extension == '.pdf':
            loader = PyPDFLoader(file_path)
        elif file_extension in ['.txt', '.md']:
            loader = TextLoader(file_path, encoding='utf-8')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        
        # åŠ è½½æ–‡æ¡£
        documents = loader.load()
        print(f"æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} é¡µ")
        
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºå°å—"""
        print(f"æ­£åœ¨åˆ†å‰²æ–‡æ¡£ï¼Œåˆ†å—å¤§å°: {settings.CHUNK_SIZE}")
        
        start_time = time.time()
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = self.text_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                'chunk_id': i,
                'chunk_size': len(chunk.page_content),
                'created_at': time.time()
            })
        
        processing_time = time.time() - start_time
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
        
        return chunks
    
    def process_document(self, file_path: str) -> List[Document]:
        """å®Œæ•´å¤„ç†æ–‡æ¡£ï¼šåŠ è½½ + åˆ†å‰² - ä¼˜åŒ–ç‰ˆæœ¬å¸¦ç¼“å­˜"""
        print(f"\n=== å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)} ===")
        
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        file_hash = self._get_file_hash(file_path)
        cached_chunks = self._load_from_cache(file_hash)
        
        if cached_chunks:
            # ä½¿ç”¨ç¼“å­˜
            filename = os.path.basename(file_path)
            for chunk in cached_chunks:
                chunk.metadata['source_file'] = filename
            
            cache_time = time.time() - start_time
            print(f"âš¡ ç¼“å­˜å‘½ä¸­: {filename}")
            print(f"   - åˆ†å—æ•°é‡: {len(cached_chunks)}")
            print(f"   - åŠ è½½è€—æ—¶: {cache_time:.2f} ç§’ (åŠ é€Ÿ ~10å€)")
            return cached_chunks
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œé‡æ–°å¤„ç†
        print("ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹å…¨æ–°å¤„ç†...")
        
        # 1. åŠ è½½æ–‡æ¡£
        print("æ­¥éª¤ 1/4: åŠ è½½æ–‡æ¡£...")
        documents = self.load_document(file_path)
        
        # 2. åˆ†å‰²æ–‡æ¡£
        print("æ­¥éª¤ 2/4: åˆ†å‰²æ–‡æ¡£...")
        chunks = self.split_documents(documents)
        
        # 3. ä¿å­˜åˆ°ç¼“å­˜
        print("æ­¥éª¤ 3/4: ä¿å­˜ç¼“å­˜...")
        self._save_to_cache(file_hash, chunks)
        
        # 4. æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°æ¯ä¸ªå—
        print("æ­¥éª¤ 4/4: æ·»åŠ å…ƒæ•°æ®...")
        filename = os.path.basename(file_path)
        file_size = os.path.getsize(file_path)
        
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                'source_file': filename,
                'file_size': file_size,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'file_hash': file_hash
            })
        
        processing_time = time.time() - start_time
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}")
        print(f"   - æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
        print(f"   - æ–‡ä»¶å“ˆå¸Œ: {file_hash[:8]}...")
        print(f"   - åŸå§‹é¡µæ•°: {len(documents)}")
        print(f"   - åˆ†å—æ•°é‡: {len(chunks)}")
        print(f"   - å¹³å‡å—å¤§å°: {sum(len(c.page_content) for c in chunks) // len(chunks)} å­—ç¬¦")
        print(f"   - å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’")
        
        return chunks

# åˆ›å»ºå…¨å±€æ–‡æ¡£åŠ è½½å™¨å®ä¾‹
document_loader = DocumentLoader()